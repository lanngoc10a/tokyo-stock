{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Prepared by Lan ngoc Nguyen. 2022. <br>\n",
    "Download data from: https://www.kaggle.com/competitions/jpx-tokyo-stock-exchange-prediction\n",
    "</p>\n",
    "\n",
    "<h2>PART 1. Data Pre-processing</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #0. Fire the system</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import modules and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #1. Read data</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/lan/Downloads/Github/Kaggle/jpx-tokyo-stock-exchange-prediction\n",
      "Training set shape == (2332531, 12)\n",
      "All timestamps == 2332531\n",
      "Featured selected: ['Open', 'High', 'Low', 'Close', 'Target']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = '/Users/lan/Downloads/Github/Kaggle/jpx-tokyo-stock-exchange-prediction/'\n",
    "\n",
    "try:\n",
    "    os.chdir(path)\n",
    "    print(\"Current working directory: {0}\".format(os.getcwd()))\n",
    "except FileNotFoundError:\n",
    "    print(\"Directory: {0} does not exist\".format(path))\n",
    "    \n",
    "# Importing Training Set\n",
    "dataset_train = pd.read_csv(\"train_files/stock_prices.csv\")\n",
    "\n",
    "# Select features (columns) to be involved intro training and predictions\n",
    "cols = ['Open', 'High', 'Low', 'Close', 'Target']\n",
    "\n",
    "# Extract dates (will be used in visualization)\n",
    "datelist_train = list(dataset_train['Date'])\n",
    "datelist_train = [dt.datetime.strptime(date, '%Y-%m-%d').date() for date in datelist_train]\n",
    "\n",
    "print('Training set shape == {}'.format(dataset_train.shape))\n",
    "print('All timestamps == {}'.format(len(datelist_train)))\n",
    "print('Featured selected: {}'.format(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowId</th>\n",
       "      <th>Date</th>\n",
       "      <th>SecuritiesCode</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>AdjustmentFactor</th>\n",
       "      <th>ExpectedDividend</th>\n",
       "      <th>SupervisionFlag</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20170104_1301</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1301</td>\n",
       "      <td>2734.0</td>\n",
       "      <td>2755.0</td>\n",
       "      <td>2730.0</td>\n",
       "      <td>2742.0</td>\n",
       "      <td>31400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.000730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20170104_1332</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1332</td>\n",
       "      <td>568.0</td>\n",
       "      <td>576.0</td>\n",
       "      <td>563.0</td>\n",
       "      <td>571.0</td>\n",
       "      <td>2798500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.012324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20170104_1333</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1333</td>\n",
       "      <td>3150.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>3210.0</td>\n",
       "      <td>270800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.006154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20170104_1376</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1376</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>1510.0</td>\n",
       "      <td>1550.0</td>\n",
       "      <td>11300</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.011053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20170104_1377</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>1377</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>3270.0</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>150800</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.003026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332526</th>\n",
       "      <td>20211203_9990</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>9990</td>\n",
       "      <td>514.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>513.0</td>\n",
       "      <td>528.0</td>\n",
       "      <td>44200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.034816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332527</th>\n",
       "      <td>20211203_9991</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>9991</td>\n",
       "      <td>782.0</td>\n",
       "      <td>794.0</td>\n",
       "      <td>782.0</td>\n",
       "      <td>794.0</td>\n",
       "      <td>35900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.025478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332528</th>\n",
       "      <td>20211203_9993</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>9993</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>1645.0</td>\n",
       "      <td>7200</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.004302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332529</th>\n",
       "      <td>20211203_9994</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>9994</td>\n",
       "      <td>2388.0</td>\n",
       "      <td>2396.0</td>\n",
       "      <td>2380.0</td>\n",
       "      <td>2389.0</td>\n",
       "      <td>6500</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.009098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2332530</th>\n",
       "      <td>20211203_9997</td>\n",
       "      <td>2021-12-03</td>\n",
       "      <td>9997</td>\n",
       "      <td>690.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>686.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>381100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0.018414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2332531 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 RowId        Date  SecuritiesCode    Open    High     Low  \\\n",
       "0        20170104_1301  2017-01-04            1301  2734.0  2755.0  2730.0   \n",
       "1        20170104_1332  2017-01-04            1332   568.0   576.0   563.0   \n",
       "2        20170104_1333  2017-01-04            1333  3150.0  3210.0  3140.0   \n",
       "3        20170104_1376  2017-01-04            1376  1510.0  1550.0  1510.0   \n",
       "4        20170104_1377  2017-01-04            1377  3270.0  3350.0  3270.0   \n",
       "...                ...         ...             ...     ...     ...     ...   \n",
       "2332526  20211203_9990  2021-12-03            9990   514.0   528.0   513.0   \n",
       "2332527  20211203_9991  2021-12-03            9991   782.0   794.0   782.0   \n",
       "2332528  20211203_9993  2021-12-03            9993  1690.0  1690.0  1645.0   \n",
       "2332529  20211203_9994  2021-12-03            9994  2388.0  2396.0  2380.0   \n",
       "2332530  20211203_9997  2021-12-03            9997   690.0   711.0   686.0   \n",
       "\n",
       "          Close   Volume  AdjustmentFactor  ExpectedDividend  SupervisionFlag  \\\n",
       "0        2742.0    31400               1.0               NaN            False   \n",
       "1         571.0  2798500               1.0               NaN            False   \n",
       "2        3210.0   270800               1.0               NaN            False   \n",
       "3        1550.0    11300               1.0               NaN            False   \n",
       "4        3330.0   150800               1.0               NaN            False   \n",
       "...         ...      ...               ...               ...              ...   \n",
       "2332526   528.0    44200               1.0               NaN            False   \n",
       "2332527   794.0    35900               1.0               NaN            False   \n",
       "2332528  1645.0     7200               1.0               NaN            False   \n",
       "2332529  2389.0     6500               1.0               NaN            False   \n",
       "2332530   696.0   381100               1.0               NaN            False   \n",
       "\n",
       "           Target  \n",
       "0        0.000730  \n",
       "1        0.012324  \n",
       "2        0.006154  \n",
       "3        0.011053  \n",
       "4        0.003026  \n",
       "...           ...  \n",
       "2332526  0.034816  \n",
       "2332527  0.025478  \n",
       "2332528 -0.004302  \n",
       "2332529  0.009098  \n",
       "2332530  0.018414  \n",
       "\n",
       "[2332531 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RowId                     0\n",
       "Date                      0\n",
       "SecuritiesCode            0\n",
       "Open                   7608\n",
       "High                   7608\n",
       "Low                    7608\n",
       "Close                  7608\n",
       "Volume                    0\n",
       "AdjustmentFactor          0\n",
       "ExpectedDividend    2313666\n",
       "SupervisionFlag           0\n",
       "Target                  238\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #2. Data pre-processing</h3>\n",
    "<p>\n",
    "Removing all commas and convert data to matrix shape format.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set == (1201, 5).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5.68000000e+02,  5.76000000e+02,  5.63000000e+02,\n",
       "         5.71000000e+02,  1.23239437e-02],\n",
       "       [ 5.72000000e+02,  5.73000000e+02,  5.65000000e+02,\n",
       "         5.68000000e+02, -2.26086957e-02],\n",
       "       [ 5.67000000e+02,  5.76000000e+02,  5.63000000e+02,\n",
       "         5.75000000e+02, -1.60142349e-02],\n",
       "       ...,\n",
       "       [ 5.68000000e+02,  5.78000000e+02,  5.68000000e+02,\n",
       "         5.74000000e+02,  2.09424084e-02],\n",
       "       [ 5.66000000e+02,  5.77000000e+02,  5.65000000e+02,\n",
       "         5.73000000e+02,  6.83760684e-03],\n",
       "       [ 5.79000000e+02,  5.85000000e+02,  5.70000000e+02,\n",
       "         5.85000000e+02, -5.60271647e-02]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataset_train = dataset_train[cols].astype(str)\n",
    "#for i in cols:\n",
    "#    for j in range(0, len(dataset_train)):\n",
    "#        dataset_train[i][j] = dataset_train[i][j].replace(',', '')\n",
    "\n",
    "#dataset_train = dataset_train.astype(float)\n",
    "\n",
    "dataset_train = dataset_train.loc[dataset_train.SecuritiesCode==1332, cols].astype(float)\n",
    "\n",
    "dataset_train.dropna(inplace=True)\n",
    "\n",
    "# Using multiple features (predictors)\n",
    "training_set = dataset_train.values\n",
    "\n",
    "print('Shape of training set == {}.'.format(training_set.shape))\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.65151433],\n",
       "       [-1.21559098],\n",
       "       [-0.86312541],\n",
       "       ...,\n",
       "       [ 1.11216035],\n",
       "       [ 0.35827658],\n",
       "       [-3.00176581]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "sc_predict = StandardScaler()\n",
    "sc_predict.fit_transform(training_set[:, 4:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1201, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape == (1140, 60, 4).\n",
      "y_train shape == (1140, 1).\n"
     ]
    }
   ],
   "source": [
    "# Creating a data structure with 90 timestamps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "n_future = 2   # Number of days we want to predict into the future\n",
    "n_past = 60     # Number of past days we want to use to predict the future\n",
    "\n",
    "for i in range(n_past, len(training_set_scaled) - n_future +1):\n",
    "    X_train.append(training_set_scaled[i - n_past:i, 0:dataset_train.shape[1] - 1])\n",
    "    y_train.append(training_set_scaled[i + n_future - 1:i + n_future, -1])\n",
    "\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "print('X_train shape == {}.'.format(X_train.shape))\n",
    "print('y_train shape == {}.'.format(y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.21559098])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_set_scaled[1:2, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 2. Create a model. Training</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #3. Building the LSTM based Neural Network</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries and packages from Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initializing the Neural Network based on LSTM\n",
    "model = Sequential()\n",
    "\n",
    "# Adding 1st LSTM layer\n",
    "model.add(LSTM(units=64, return_sequences=True, input_shape=(n_past, dataset_train.shape[1]-1)))\n",
    "\n",
    "# Adding 2nd LSTM layer\n",
    "model.add(LSTM(units=10, return_sequences=False))\n",
    "\n",
    "# Adding Dropout\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "# Compiling the Neural Network\n",
    "model.compile(optimizer = Adam(learning_rate=0.01), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #4. Start training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8/8 [==============================] - ETA: 0s - loss: 1.0886\n",
      "Epoch 1: val_loss improved from inf to 1.11147, saving model to weights.h5\n",
      "8/8 [==============================] - 4s 160ms/step - loss: 1.0886 - val_loss: 1.1115 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0679\n",
      "Epoch 2: val_loss improved from 1.11147 to 0.96398, saving model to weights.h5\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0581 - val_loss: 0.9640 - lr: 0.0100\n",
      "Epoch 3/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0316\n",
      "Epoch 3: val_loss did not improve from 0.96398\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0290 - val_loss: 0.9811 - lr: 0.0100\n",
      "Epoch 4/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0369\n",
      "Epoch 4: val_loss improved from 0.96398 to 0.96089, saving model to weights.h5\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0218 - val_loss: 0.9609 - lr: 0.0100\n",
      "Epoch 5/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0119\n",
      "Epoch 5: val_loss did not improve from 0.96089\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0150 - val_loss: 0.9687 - lr: 0.0100\n",
      "Epoch 6/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0356\n",
      "Epoch 6: val_loss did not improve from 0.96089\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0278 - val_loss: 0.9617 - lr: 0.0100\n",
      "Epoch 7/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0222\n",
      "Epoch 7: val_loss improved from 0.96089 to 0.95693, saving model to weights.h5\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 1.0120 - val_loss: 0.9569 - lr: 0.0100\n",
      "Epoch 8/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.9579\n",
      "Epoch 8: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0061 - val_loss: 0.9607 - lr: 0.0100\n",
      "Epoch 9/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0129\n",
      "Epoch 9: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0042 - val_loss: 0.9585 - lr: 0.0100\n",
      "Epoch 10/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0149\n",
      "Epoch 10: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0046 - val_loss: 0.9603 - lr: 0.0100\n",
      "Epoch 11/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0060\n",
      "Epoch 11: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0026 - val_loss: 0.9613 - lr: 0.0100\n",
      "Epoch 12/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0095\n",
      "Epoch 12: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0068 - val_loss: 0.9640 - lr: 0.0100\n",
      "Epoch 13/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 0.9979\n",
      "Epoch 13: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0022 - val_loss: 0.9624 - lr: 0.0100\n",
      "Epoch 14/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0025\n",
      "Epoch 14: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.0028 - val_loss: 0.9603 - lr: 0.0100\n",
      "Epoch 15/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0110\n",
      "Epoch 15: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0036 - val_loss: 0.9591 - lr: 0.0100\n",
      "Epoch 16/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0060\n",
      "Epoch 16: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 1.0035 - val_loss: 0.9596 - lr: 0.0100\n",
      "Epoch 17/30\n",
      "7/8 [=========================>....] - ETA: 0s - loss: 1.0088\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.95693\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 1.0020 - val_loss: 0.9608 - lr: 0.0100\n",
      "Epoch 17: early stopping\n",
      "CPU times: user 18.7 s, sys: 5.09 s, total: 23.8 s\n",
      "Wall time: 9.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=1e-10, patience=10, verbose=1)\n",
    "rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, verbose=1)\n",
    "mcp = ModelCheckpoint(filepath='weights.h5', monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "tb = TensorBoard('logs')\n",
    "\n",
    "history = model.fit(X_train, y_train, shuffle=True, epochs=30, callbacks=[es, rlr, mcp, tb], validation_split=0.2, verbose=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    #plt.plot(history.history['val_out_seq_loss'], label='val_out_seq_loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "Notes:<br>\n",
    "<ul>\n",
    "<li><b>EarlyStopping</b> - Stop training when a monitored metric has stopped improving.</li>\n",
    "<li><code>monitor</code> - quantity to be monitored.</li>\n",
    "<li><code>min_delta</code> - minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than <code>min_delta</code>, will count as no improvement.</li>\n",
    "<li><code>patience</code> - number of epochs with no improvement after which training will be stopped.</li>\n",
    "</ul>\n",
    "\n",
    "<ul>\n",
    "<li><b>ReduceLROnPlateau</b> - Reduce learning rate when a metric has stopped improving.</li>\n",
    "<li><code>factor</code> - factor by which the learning rate will be reduced. <code>new_lr = lr * factor</code>.</li>\n",
    "</ul>\n",
    "</p>\n",
    "\n",
    "<hr>\n",
    "\n",
    "<p>\n",
    "The last date for our training set is <code>30-Dec-2016</code>.<br>\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "We will perform predictions for the next <b>20</b> days, since <b>2017-01-01</b> to <b>2017-01-20</b>.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>PART 3. Make future predictions</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list of sequence of days for predictions\n",
    "datelist_future = pd.date_range(datelist_train[-1], periods=n_future, freq='1d').tolist()\n",
    "\n",
    "'''\n",
    "Remeber, we have datelist_train from begining.\n",
    "'''\n",
    "\n",
    "# Convert Pandas Timestamp to Datetime object (for transformation) --> FUTURE\n",
    "datelist_future_ = []\n",
    "for this_timestamp in datelist_future:\n",
    "    datelist_future_.append(this_timestamp.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #5. Make predictions for future dates</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform predictions\n",
    "predictions_train = model.predict(X_train[n_past:])\n",
    "\n",
    "predictions_future = model.predict(X_train[-n_future:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected 1080 rows, received array of length 2332410",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-045828f4f517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mPREDICTIONS_FUTURE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_future\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatelist_future\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mPREDICTION_TRAIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatelist_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn_past\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn_future\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mset_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   4768\u001b[0m                 \u001b[0;31m# check newest element against length of calling frame, since\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4769\u001b[0m                 \u001b[0;31m# ensure_index_from_sequences would not raise for append=False.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4770\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   4771\u001b[0m                     \u001b[0;34mf\"Length mismatch: Expected {len(self)} rows, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4772\u001b[0m                     \u001b[0;34mf\"received array of length {len(arrays[-1])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected 1080 rows, received array of length 2332410"
     ]
    }
   ],
   "source": [
    "# Inverse the predictions to original measurements\n",
    "\n",
    "# ---> Special function: convert <datetime.date> to <Timestamp>\n",
    "def datetime_to_timestamp(x):\n",
    "    '''\n",
    "        x : a given datetime value (datetime.date)\n",
    "    '''\n",
    "    return datetime.strptime(x.strftime('%Y%m%d'), '%Y%m%d')\n",
    "\n",
    "\n",
    "y_pred_future = sc_predict.inverse_transform(predictions_future)\n",
    "y_pred_train = sc_predict.inverse_transform(predictions_train)\n",
    "\n",
    "PREDICTIONS_FUTURE = pd.DataFrame(y_pred_future, columns=['Target']).set_index(pd.Series(datelist_future))\n",
    "PREDICTION_TRAIN = pd.DataFrame(y_pred_train, columns=['Target']).set_index(pd.Series(datelist_train[2 * n_past + n_future -1:]))\n",
    "\n",
    "# Convert <datetime.date> to <Timestamp> for PREDCITION_TRAIN\n",
    "PREDICTION_TRAIN.index = PREDICTION_TRAIN.index.to_series().apply(datetime_to_timestamp)\n",
    "\n",
    "PREDICTION_TRAIN.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Step #6. Visualize the Predictions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot size \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 14, 5\n",
    "\n",
    "# Plot parameters\n",
    "START_DATE_FOR_PLOTTING = '2012-06-01'\n",
    "\n",
    "plt.plot(PREDICTIONS_FUTURE.index, PREDICTIONS_FUTURE['Open'], color='r', label='Predicted Stock Price')\n",
    "plt.plot(PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:].index, PREDICTION_TRAIN.loc[START_DATE_FOR_PLOTTING:]['Open'], color='orange', label='Training predictions')\n",
    "plt.plot(dataset_train.loc[START_DATE_FOR_PLOTTING:].index, dataset_train.loc[START_DATE_FOR_PLOTTING:]['Open'], color='b', label='Actual Stock Price')\n",
    "\n",
    "plt.axvline(x = min(PREDICTIONS_FUTURE.index), color='green', linewidth=2, linestyle='--')\n",
    "\n",
    "plt.grid(which='major', color='#cccccc', alpha=0.5)\n",
    "\n",
    "plt.legend(shadow=True)\n",
    "plt.title('Predcitions and Acutal Stock Prices', family='Arial', fontsize=12)\n",
    "plt.xlabel('Timeline', family='Arial', fontsize=10)\n",
    "plt.ylabel('Stock Price Value', family='Arial', fontsize=10)\n",
    "plt.xticks(rotation=45, fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parse training set timestamp for better visualization\n",
    "dataset_train = pd.DataFrame(dataset_train, columns=cols)\n",
    "dataset_train.index = datelist_train\n",
    "dataset_train.index = pd.to_datetime(dataset_train.index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
